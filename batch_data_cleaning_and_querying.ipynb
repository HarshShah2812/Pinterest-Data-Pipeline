{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3872a3-d654-467e-8962-889cea5adcbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking the contents in FileStore, the location where we uploaded our AWS credentials\n",
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7711d78f-0137-4dde-af99-42fc250f7933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c762bd1-54da-4099-a185-45f919b12ef9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a47dc75-50af-4501-8049-f980f47149e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff516508-e8f0-4c48-8064-e2f1c255f31c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AWS_S3_BUCKET = \"user-0e4c2ab6fb3b-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/pdp_mount\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3fc4ae4-dcf0-4832-ab61-00e691a2572e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the S3 bucket was mounted succesfully\n",
    "display(dbutils.fs.ls(\"/mnt/pdp_mount/../..\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682c5abc-5af6-4504-8abf-d5762ccb42e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the whole path to check the contents of a given topic in the S3 bucket\n",
    "display(dbutils.fs.ls(\"/mnt/pdp_mount/topics/0e4c2ab6fb3b.user/partition=0/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151769c7-f209-4fab-9c7d-61af7c53c914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/pdp_mount/topics/0e4c2ab6fb3b.pin/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_pin = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e298fa-f711-474c-92fc-d22793029232",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/pdp_mount/topics/0e4c2ab6fb3b.geo/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_geo = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751a6725-42ac-4341-9e2a-b70d01d8bc33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/pdp_mount/topics/0e4c2ab6fb3b.user/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_user = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1169e26a-bac4-427e-94d0-c5194e10f1ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1) Clean the DataFrame that contains information about Pinterest posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16750e19-75ab-43ea-8179-36a02253befd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace empty entries and entries with no relevant data in each column with Nones\n",
    "df_pin = df_pin.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "\n",
    "df_pin = df_pin.withColumn(\"follower_count\", when(df_pin.follower_count.contains(\"User Info Error\"), 0).otherwise(df_pin.follower_count))\n",
    "\n",
    "df_pin = df_pin.withColumn(\"poster_name\", when(df_pin.poster_name.contains(\"User Info Error\"), None).otherwise(df_pin.poster_name))\n",
    "\n",
    "df_pin = df_pin.select([when(col(c).contains(\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\"), None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "\n",
    "df_pin = df_pin.select([when(col(c).contains(\"Image src error\"), None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "\n",
    "df_pin = df_pin.select([when(col(c).contains(\"No description\"), None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "\n",
    "df_pin = df_pin.select([when(col(c).contains(\"Untitled\"), None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "\n",
    "df_pin = df_pin.select([when(col(c).contains(\"No Title Data Available\"), None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "df_pin = df_pin.withColumn('follower_count', when(df_pin.follower_count.endswith('k'), regexp_replace(df_pin.follower_count, 'k', '000')) \\\n",
    "    .when(df_pin.follower_count.endswith('M'), regexp_replace(df_pin.follower_count, 'M', '000000')) \\\n",
    "    .otherwise(df_pin.follower_count))\n",
    "\n",
    "# change the datatype of the \"follower_count\" column to int\n",
    "df_pin = df_pin.withColumn(\"follower_count\",df_pin.follower_count.cast('int'))\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn('save_location', when(df_pin.save_location.startswith('Local save in '), regexp_replace(df_pin.save_location, 'Local save in ', '')))\n",
    "\n",
    "# Rename the index column to ind\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_pin = df_pin.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bfad80-477f-4cac-a6aa-eb440d3f9356",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2) Clean the DataFrame that contains information about geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0147d93a-a7f8-4c8e-ba76-201418373e94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "# Drop the latitude and longitude columns from the DataFrame\n",
    "# Reorder the DataFrame columns\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(\"latitude\", \"longitude\")) \\\n",
    ".select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b44654-772e-42d7-bb1a-1acf8435ff02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3) Clean the DataFrame that contains information about users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1502658d-ae0e-430b-b71a-23b3987cf6a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "# Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.withColumn(\"user_name\", concat(\"first_name\", \"last_name\")) \\\n",
    "    .select(\"age\", \"date_joined\", \"ind\", \"user_name\")\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "# Reorder the DataFrame columns\n",
    "df_user = df_user.withColumn(\"date_joined\", to_timestamp(\"date_joined\")) \\\n",
    "    .select(\"ind\", \"user_name\", \"age\", \"date_joined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2637178-22e4-49cb-937b-8fa5b3e4a712",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4) Find the most popular category in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f16109-6486-4c4d-b88f-ad508e76462d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pin.createOrReplaceTempView(\"PIN\")\n",
    "df_geo.createOrReplaceTempView(\"GEO\")\n",
    "df_user.createOrReplaceTempView(\"USER\")\n",
    "df_most_popular_cat_by_country = df_pin.join(df_geo, on = \"ind\") \\\n",
    "    .groupBy('country', 'category') \\\n",
    "        .agg(count('*').alias('count')) \\\n",
    "            .groupBy('country') \\\n",
    "                .agg(max(struct('count', 'category')).alias('max_count')) \\\n",
    "                    .select('country', 'max_count.category', 'max_count.count') \\\n",
    "                        .withColumnRenamed(\"count\", \"category_count\")\n",
    "\n",
    "df_most_popular_cat_by_country.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffecf49-3833-4d3e-b859-1e6b02ad443c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "5) Find which was the most popular category each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8709b46f-2f07-428e-8c73-5acb2ee80671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_most_popular_category_by_year = df_pin.join(df_geo, on = 'ind') \\\n",
    "    .filter((year('timestamp') >= 2018) & (year('timestamp') <= 2022))\n",
    "\n",
    "df_most_popular_category_by_year = df_most_popular_category_by_year.groupBy(year('timestamp').alias('post_year'), 'category') \\\n",
    "    .agg(count('*').alias('category_count'))\n",
    "\n",
    "df_most_popular_category_by_year = df_most_popular_category_by_year.orderBy(['post_year', 'category_count'], ascending = [True, False]) \\\n",
    "    .groupBy('post_year') \\\n",
    "        .agg({'category': 'first', 'category_count': 'first'}) \\\n",
    "            .select('post_year', 'first(category)', 'first(category_count)') \\\n",
    "                .withColumnRenamed('first(category)', 'category') \\\n",
    "                    .withColumnRenamed('first(category_count)', 'category_count')\n",
    "\n",
    "df_most_popular_category_by_year.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12be9e39-8125-464f-8580-b63268701487",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6) Find the user with most followers in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c201309b-778e-4227-b71d-66ae4920ca56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_most_followers_by_country = df_pin.join(df_geo, on = 'ind') \\\n",
    "    .groupBy('country', 'poster_name') \\\n",
    "        .agg(max('follower_count')) \\\n",
    "            .withColumn('rank', rank().over(Window.partitionBy('country').orderBy(desc('max(follower_count)')))) \\\n",
    "                .filter('rank = 1') \\\n",
    "                    .select('country', 'poster_name', 'max(follower_count)') \\\n",
    "                        .withColumnRenamed('max(follower_count)', 'follower_count')\n",
    "\n",
    "df_most_followers_by_country.show()\n",
    "\n",
    "df_country_with_most_followed_user = df_most_followers_by_country.groupBy('country') \\\n",
    "    .agg(max('follower_count').alias('follower_count')) \\\n",
    "        .orderBy(desc('follower_count')) \\\n",
    "            .limit(1) \\\n",
    "                .select('country', 'follower_count')\n",
    "\n",
    "df_country_with_most_followed_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65aacec1-5aa0-44a7-b655-15fd84cd6fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7) Find the most popular category for different age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6911dd7-ab94-4a9e-9684-0213e1bcda34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_user_age_group = df_pin.join(df_user, on = 'ind') \\\n",
    "    .withColumn(\"age_group\", \n",
    "                when(df_user.age.between(18, 24), \"18-24\")\n",
    "                .when(df_user.age.between(25, 35), \"25-35\")\n",
    "                .when(df_user.age.between(36, 50), \"36-50\")\n",
    "                .when(df_user.age > 50, \"50+\"))\n",
    "\n",
    "df_category_count_by_age = df_user_age_group.groupBy(\"age_group\", \"category\") \\\n",
    "    .agg(count(\"*\").alias(\"category_count\")) \\\n",
    "        .groupBy(\"age_group\") \\\n",
    "            .agg(max(struct(\"category_count\", \"category\")).alias(\"max_count\")) \\\n",
    "                .select(\"age_group\", \"max_count.category\", \"max_count.category_count\")\n",
    "\n",
    "df_category_count_by_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9a1c20-66e8-4286-85f9-317f2ca3b181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8) Find the median follower count for different age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1f997c-82f1-4404-ab92-912592157a90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_median_follower_count_by_age = df_user_age_group.groupBy(\"age_group\") \\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5, lit(1000000)).alias(\"median_follower_count\")) \\\n",
    "        .select(\"age_group\", \"median_follower_count\")\n",
    "\n",
    "df_median_follower_count_by_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3e29c2-e9e6-4b6d-a88d-f53e301bdfd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "9) Find how many users have joined each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe62708-b5e8-4483-bcc5-b57048100b45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_user_geo = df_user.join(df_geo, on = \"ind\") \\\n",
    "    .select(year(\"timestamp\").alias(\"post_year\"), 'date_joined') \\\n",
    "        .where((df_user.date_joined >= '2015-01-01') & (df_user.date_joined < '2021-01-01'))\n",
    "\n",
    "df_user_geo.show()\n",
    "\n",
    "df_users_joined_by_year = df_user_geo.groupBy(\"post_year\") \\\n",
    "   .agg(count(\"*\").alias(\"number_users_joined\")) \\\n",
    "        .sort(asc(\"post_year\")) \\\n",
    "            .select(\"post_year\", \"number_users_joined\")\n",
    "\n",
    "df_users_joined_by_year.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90b8d56-3563-4ad6-b3ec-c7803818a9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "10) Find the median follower count of users based on their joining year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35fe957-9270-43db-82ba-f021cc019b8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_user_geo_pin = df_user.join(df_geo, on = \"ind\").join(df_pin, on = \"ind\") \\\n",
    "    .select(year(\"timestamp\").alias(\"post_year\"), 'date_joined', \"follower_count\") \\\n",
    "        .where((df_user.date_joined >= '2015-01-01') & (df_user.date_joined < '2021-01-01'))\n",
    "\n",
    "df_user_median_follower_count = df_user_geo_pin.groupBy(\"post_year\") \\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5, lit(1000000)).alias(\"median_follower_count\")) \\\n",
    "        .select(\"post_year\", \"median_follower_count\")\n",
    "\n",
    "df_user_median_follower_count.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ae5814-aa59-49ee-89e2-d57653d8795a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "11) Find the median follower count of users based on their joining year and age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f3c545-bc66-4072-a5eb-3e188d6784e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_user_geo_pin = df_user.join(df_geo, on = \"ind\").join(df_pin, on = \"ind\") \\\n",
    "    .withColumn(\"age_group\", \n",
    "                when(df_user.age.between(18, 24), \"18-24\")\n",
    "                .when(df_user.age.between(25, 35), \"25-35\")\n",
    "                .when(df_user.age.between(36, 50), \"36-50\")\n",
    "                .when(df_user.age > 50, \"50+\")) \\\n",
    "        .select(\"age_group\", year(\"timestamp\").alias(\"post_year\"), 'date_joined', \"follower_count\") \\\n",
    "            .where((df_user.date_joined >= '2015-01-01') & (df_user.date_joined < '2021-01-01'))\n",
    "\n",
    "df_median_follower_by_year_joined = df_user_geo_pin.groupBy(\"age_group\", \"post_year\") \\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5, lit(1000000)).alias(\"median_follower_count\")) \\\n",
    "        .select(\"age_group\", \"post_year\", \"median_follower_count\")\n",
    "\n",
    "df_median_follower_by_year_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aafd50f-72e6-4935-a7eb-7d4caf9228d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount the bucket\n",
    "dbutils.fs.unmount(MOUNT_NAME)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "batch_data_cleaning_and_querying",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PDPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
