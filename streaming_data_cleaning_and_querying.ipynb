{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c2571e-aeb2-4895-a67c-cf0b3f299258",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read data from Kinesis streams in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f04ba9-255e-42b2-8bab-a81e0adb5a9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe81474c-7633-476d-8df4-4d006b171a07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cec42b1-34fc-4bc6-8351-85741822a7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "    \n",
    "def create_streaming_dataframe(record_type):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    record_type: str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: \n",
    "        Dataframe containing streaming data\n",
    "    '''\n",
    "    # Read data from Kinesis pin stream using structured streaming\n",
    "    df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', f'streaming-0e4c2ab6fb3b-{record_type}') \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option(\"format\", \"json\") \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load()\n",
    "    \n",
    "    # Deserialise the \"data\" column of the dataframe to see the data contained in the stream\n",
    "    df = df.selectExpr(\"CAST(data as STRING)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# query = streaming_df_pin.writeStream.format(\"console\").start()\n",
    "\n",
    "def create_delta_table(df, record_type):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        Dataframe that has been cleaned and transformed\n",
    "    \n",
    "    record_type: str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    delta_table:    \n",
    "        Delta table containing streaming data\n",
    "    '''\n",
    "    delta_table = df.writeStream \\\n",
    "   .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"/tmp/kinesis/_checkpoints/\") \\\n",
    "    .table(f\"0e4c2ab6fb3b_{record_type}_table\")\n",
    "\n",
    "    return delta_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369efdb7-d3b3-47a1-90ed-e947a213e860",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transform the Kinesis streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d55811b-24b5-4767-b775-0bb5aaeaf51f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace empty entries and entries with no relevant data in each column with Nones\n",
    "def transform_pin_data(df):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        Dataframe containing information about Pinterest posts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_cleaned:\n",
    "        Cleaned version of streaming_df_pin  \n",
    "    '''\n",
    "    df = df.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "    df = df.withColumn(\"follower_count\", when(df.follower_count.contains(\"User Info Error\"), 0).otherwise(df.follower_count))\n",
    "\n",
    "    df = df.withColumn(\"poster_name\", when(df.poster_name.contains(\"User Info Error\"), None).otherwise(df.poster_name))\n",
    "\n",
    "    df = df.select([when(col(c).contains(\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\"), None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "    df = df.select([when(col(c).contains(\"Image src error\"), None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "    df = df.select([when(col(c).contains(\"No description\"), None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "    df = df.select([when(col(c).contains(\"Untitled\"), None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "    df = df.select([when(col(c).contains(\"No Title Data Available\"), None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "    # Ensure that each column containing numeric data has a numeric data type\n",
    "    df = df.withColumn('follower_count', when(df.follower_count.endswith('k'), regexp_replace(df.follower_count, 'k', '000')) \\\n",
    "        .when(df.follower_count.endswith('M'), regexp_replace(df.follower_count, 'M', '000000')) \\\n",
    "        .otherwise(df.follower_count))\n",
    "\n",
    "    # Change the datatype of the \"follower_count\" column to integer\n",
    "    df = df.withColumn(\"follower_count\", df.follower_count.cast('int'))\n",
    "\n",
    "    # Clean the data in the save_location column to include only the save location path\n",
    "    df = df.withColumn('save_location', when(df.save_location.startswith('Local save in '), regexp_replace(df.save_location, 'Local save in ', '')))\n",
    "\n",
    "    # Rename the index column to ind\n",
    "    df = df.withColumnRenamed('index', 'ind')\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    df_cleaned = df.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f02c828-235a-489e-82cf-7fe6848b2e28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_geo_data(df):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        Dataframe containing information about Geolocation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_cleaned:\n",
    "        Cleaned version of streaming_df_geo  \n",
    "    '''\n",
    "    # Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "    # Drop the latitude and longitude columns from the DataFrame\n",
    "    # Reorder the DataFrame columns\n",
    "    df = df.withColumn(\"coordinates\", array(\"latitude\", \"longitude\")) \\\n",
    "    .select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "    # Convert the timestamp column from a string to a timestamp data type\n",
    "    df_cleaned = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1c3e73-7e3e-44a7-956e-ec9c3e8d019c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_user_data(df):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    streaming_df_user:\n",
    "        Dataframe containing information about users\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    streaming_df_user_cleaned:\n",
    "        Cleaned version of streaming_df_user  \n",
    "    '''\n",
    "    # Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "    # Drop the first_name and last_name columns from the DataFrame\n",
    "    df = df.withColumn(\"user_name\", concat(\"first_name\", \"last_name\")) \\\n",
    "        .select(\"age\", \"date_joined\", \"ind\", \"user_name\")\n",
    "\n",
    "    # Convert the date_joined column from a string to a timestamp data type\n",
    "    # Reorder the DataFrame columns\n",
    "    df_cleaned = df.withColumn(\"date_joined\", to_timestamp(\"date_joined\")) \\\n",
    "        .select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c209566-9a12-41cf-b34e-d1c876e48ae3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write the streaming data to Delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fbdc5b-225a-470b-abe9-4dfcd945eef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[105]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fd72ec439a0&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[105]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fd72ec439a0&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the structure of the Pinterest dataframe \n",
    "schema = StructType([\n",
    "        StructField(\"index\", IntegerType()),\n",
    "        StructField(\"unique_id\", StringType()),\n",
    "        StructField(\"title\", StringType()),\n",
    "        StructField(\"description\", StringType()),\n",
    "        StructField(\"poster_name\", StringType()),\n",
    "        StructField(\"follower_count\", StringType()),\n",
    "        StructField(\"tag_list\", StringType()),\n",
    "        StructField(\"is_image_or_video\", StringType()),\n",
    "        StructField(\"image_src\", StringType()),\n",
    "        StructField(\"downloaded\", IntegerType()),\n",
    "        StructField(\"save_location\", StringType()),\n",
    "        StructField(\"category\", StringType())\n",
    "    ])\n",
    "\n",
    "df_pin = create_streaming_dataframe(\"pin\")\n",
    "df_pin = df_pin.withColumn(\"cast_data\", from_json(col(\"data\"), schema)) \\\n",
    "    .select(\"cast_data.*\")\n",
    "\n",
    "df_pin.columns\n",
    "\n",
    "transformed_df_pin = transform_pin_data(df_pin)\n",
    "create_delta_table(transformed_df_pin, \"pin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966eef04-d78c-496e-8b3f-973172f38f01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The following block of code above firstly creates the dataframe containing the streaming Pinterest data. The \"data\" column is then named \"cast_data\" and the column expression that is used is the \"data\" column itself, which is parsed from the corresponding JSON string, and the schema that has been defined above is the schema used; finally, all the columns within \"cast_data\" are selected. The dataframe is then transformed using the transform_pin_data() function created earlier, and is given the name \"transformed_df_pin\", which is then used to create a delta table using the create_delta_table() function created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6ff43b-0f83-4ef2-aab0-69ff7eb21ba3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[107]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fd72f4d5f70&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[107]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fd72f4d5f70&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the structure of the Geolocation dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df_geo = create_streaming_dataframe(\"geo\")\n",
    "df_geo = df_geo.withColumn(\"cast_data\", from_json(col(\"data\"), schema)) \\\n",
    "    .select(\"cast_data.*\")\n",
    "\n",
    "transformed_df_geo = transform_geo_data(df_geo)\n",
    "create_delta_table(transformed_df_geo, \"geo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db6ef68-8d96-42c7-9430-c95a0d111f20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The following block of code above firstly creates the dataframe containing the streaming Geolocation data. The \"data\" column is then named \"cast_data\" and the column expression that is used is the \"data\" column itself, which is parsed from the corresponding JSON string, and the schema that has been defined above is the schema used; finally, all the columns within \"cast_data\" are selected. The dataframe is then transformed using the transform_geo_data() function created earlier, and is given the name \"transformed_df_geo\", which is then used to create a delta table using the create_delta_table() function created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b3e811-0cc9-4a53-a964-0c34a1e1e1a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[104]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fd72ec43910&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[104]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fd72ec43910&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the structure of the User dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"date_joined\", StringType())\n",
    "])\n",
    "\n",
    "df_user = create_streaming_dataframe(\"user\")\n",
    "df_user = df_user.withColumn(\"cast_data\", from_json(col(\"data\"), schema)) \\\n",
    "    .select(\"cast_data.*\")\n",
    "\n",
    "transformed_df_user = transform_user_data(df_user)\n",
    "create_delta_table(transformed_df_user, \"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d12d8b-aa37-4518-bf88-e0a51c9b3d5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The following block of code above firstly creates the dataframe containing the streaming User data. The \"data\" column is then named \"cast_data\" and the column expression that is used is the \"data\" column itself, which is parsed from the corresponding JSON string, and the schema that has been defined above is the schema used; finally, all the columns within \"cast_data\" are selected. The dataframe is then transformed using the transform_user_data() function created earlier, and is given the name \"transformed_df_user\", which is then used to create a delta table using the create_delta_table() function created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1b016a-7a6c-4836-9f28-dfcc4e1d6757",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[108]: True</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[108]: True</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f96cdd-378f-45d5-b235-1400b30a7040",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The command above deletes the checkpoint folder that is created that allows you to recover the previous state of a query in case of failure. This is necessary in order to write the streaming data to data tables again."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming_data_cleaning_and_querying",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
